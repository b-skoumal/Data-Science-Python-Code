{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Science, CS 5963 / Math 3900\n",
    "\n",
    "## Lab 19: Parallel Computing, MapReduce, and Spark. \n",
    "\n",
    "In this lab, we'll discuss parallel computing with a focus on MapReduce and Spark. \n",
    "\n",
    "**Further reading:**\n",
    "\n",
    "[J. Lin and C. Dyer, Data-Intensive Text Processing with MapReduce (2010)](https://lintool.github.io/MapReduceAlgorithms/MapReduce-book-final.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is parallel computing? \n",
    "\n",
    "*Parallel computing* is when a computational task is broken into smaller subtasks, which are  processed simultaneously and indpendently, typically by multiple processors or computers. \n",
    "\n",
    "Of course, the benefits of parallel computing are that either larger tasks can be completed or tasks can be completed faster. \n",
    "\n",
    "**Example:** In homework 4, to use cross validation to search for optimal parameters, you could have put each parameter tested on a different computer. See the n_jobs parameter in the scikit-learn command [*GridSearchCV*](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html).\n",
    "\n",
    "On the other hand, parallel computing is not easy. There are a large number of low-level programming aspects that must be handled. For example, one must consider  \n",
    "- Partitioning input data\n",
    "- Shared memory (Open Multiprocessing (OpenMP)) or distributed memory (Message Passing Interface (MPI)) architectures\n",
    "- Scheduling execution\n",
    "- Handling failures\n",
    "- Interprocessor communication\n",
    "\n",
    "There are a lot of difficult Computer Science questions here! For example, [Amdahl's law](https://en.wikipedia.org/wiki/Amdahl%27s_law) gives the theoretical speedup of a process depending on how much of the task can be parallelized. \n",
    "\n",
    "Python has some support for low-level parallelism; see the \n",
    "[python documentation](https://docs.python.org/3.6/library/concurrency.html), \n",
    "[multiprocessing](https://docs.python.org/3.6/library/multiprocessing.html),\n",
    "[ipyparallel](https://ipyparallel.readthedocs.io/en/latest/index.html), \n",
    "[joblib](https://pypi.python.org/pypi/joblib), \n",
    "*etc*...\n",
    "\n",
    "Today, I'll just discuss some parallelization tools built for data analysis, especially,  MapReduce. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MapReduce\n",
    "\n",
    "1. Programming model for  distributed computations\n",
    "+ Addressing large data sets (think ~ 1 terabyte of data)\n",
    "+ Parallel and distributed algorithm\n",
    "+ Cluster framework\n",
    "+ Functional\n",
    "\n",
    "History:\n",
    "\n",
    "1. Developed by Google, but built on previously-developed ideas\n",
    "+  Apache Hadoop is an open source implemenation \n",
    "+ implemented in Java\n",
    "+ There are several Python interfaces to Hadoop, including MrJob, etc.... Unfortunately, these don't yet work nicely with Jupyter notebook. \n",
    "\n",
    "For tasks that have a particular structure, MapReduce is fairly easy to use.\n",
    "\n",
    "- See Lecture 14 of [Harvard CS109 notes](https://drive.google.com/drive/folders/0BxYkKyLxfsNVd0xicUVDS1dIS0k)\n",
    "- See the accompanying video [here](http://cs109.github.io/2015/pages/videos.html).\n",
    "\n",
    "**Famous example: word count**\n",
    "\n",
    "[Google Ngram viewer](https://books.google.com/ngrams) looks at word frequencies of all google books\n",
    "\n",
    "**Exercise:** How would you use MapReduce to find anagrams?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark\n",
    "\n",
    "Spark can be thought of as  MapReduce 2.0\n",
    "\n",
    "- In memory as opposed to disk\n",
    "- Data can be cached in memory or disk for future use\n",
    "- 100x faster than Hadoop MapReduce in memory or 10x faster on disk\n",
    "- resilient distributed dataset (RDD)\n",
    "- Python, Java, and Scala interfaces\n",
    "- [apache-spark](http://spark.apache.org/) can be used in python through [findspark](https://github.com/minrk/findspark)\n",
    "- Easier than Hadoop while being functional, runs a general DAG\n",
    "\n",
    "For more, see Lecture 15 of [Harvard CS109 notes](https://drive.google.com/drive/folders/0BxYkKyLxfsNVd0xicUVDS1dIS0k) \n",
    "and accompanying [notebook](https://github.com/cs109/2015/blob/master/Lectures/15b-Spark.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark must be installed seperately than Anaconda. For Mac users, try\n",
    "\n",
    "```\n",
    "$ brew install apache-spark\n",
    "$ pip install findspark\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:** Compute the sum $\\sum_{i=1}^{10} i^2$ \n",
    "\n",
    "We could do this using numpy as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "385"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using pyspark, here's how to evaluate the sum. The second parameter below specifies the number of partitions to cut the dataset into. Spark will run one task for each partition. Typically, you can choose 2-4 partitions for each CPU in your cluster. Spark will automatically pick this number if you don't specify  it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "385"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10],4).map(lambda x: x**2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can count the words in a list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cat', 2), ('elephant', 1), ('rat', 2)]\n"
     ]
    }
   ],
   "source": [
    "wordsList = ['cat', 'elephant', 'rat', 'rat', 'cat']\n",
    "wordsRDD = sc.parallelize(wordsList, 4)      \n",
    "wordCountsCollected = (wordsRDD\n",
    "                       .map(lambda w: (w, 1))\n",
    "                       .reduceByKey(lambda x,y: x+y)\n",
    "                       .collect())\n",
    "print(wordCountsCollected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon Web Service (AWS)\n",
    "\n",
    "[Amazon Web Service (AWS)](https://aws.amazon.com/) offeres cloud-computing services that make up an on-demand computing platform.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
