{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Science, CS 5963 / Math 3900\n",
    "## Lab 17: Natural Language Processing (NLP)\n",
    "\n",
    "In this lab, we'll do a sentiment analysis for movie reviews. For this purpose, we'll introduce  the [Natural Language Toolkit (NLTK)](http://www.nltk.org/), a python library for  Natural Language Processing. \n",
    "\n",
    "**Further reading:** \n",
    "\n",
    "[S. Bird, E. Klein, and E. Loper, *Natural Language Processing with Python - Analyzing Text with the Natural Language Toolkit*](http://www.nltk.org/book/). \n",
    "\n",
    "\n",
    "[C. Manning and H. SchÃ¼tze, *Foundations of Statistical Natural Language Processing* (1999).](http://nlp.stanford.edu/fsnlp/)\n",
    "\n",
    "[D. Jurafsky and J. H. Martin, *Speech and Language Processing* (2016).](https://web.stanford.edu/~jurafsky/slp3/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall \n",
    "**Last lecture:** Guest lecturer Vivek Srikumar gave a nice overview of Natural Language Processing (NLP). He gave several examples of NLP tasks: \n",
    "1. Part of speech tagging\n",
    "+ Information Extraction\n",
    "+ Sentiment Analysis\n",
    "+ Semantic Parsing\n",
    "\n",
    "One of the major takeaways from his talk is that the current state-of-the-art for many NLP tasks is to find a good way to represent the text (\"extract features\") and then to use machine learning / statistics tools, such as classification or clustering. \n",
    "\n",
    "Our goal today is to use NLTK + scikit-learn to do some basic NLP tasks.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install datasets and models\n",
    "\n",
    "To use NLTK, you must first download and install the datasets and models. I couldn't do this directly from a Jupyter notebook and instead had to go to the command line and enter:\n",
    "\n",
    "```python\n",
    "$ python3\n",
    ">>> import nltk\n",
    ">>> nltk.download('all')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports and setup\n",
    "import nltk\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (15, 9)\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics of NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching text\n",
    "\n",
    "The text of Monty Python and the Holy Grail has 16,967 words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(text6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word \"swallow\" appears 10 times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text6.count(\"swallow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might want to know the context in which \"swallow\" appears in the text\n",
    "\n",
    "\"You shall know a word by the company it keeps.\"    ---John Firth\n",
    "\n",
    "Use the `concordance` function to print out the words just before and after all occurrences of the word \"swallow\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text6.concordance(\"swallow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see what other words appear in the same context using the  `similar` function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text6.similar(\"african\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that 'african' and 'unladen' both appeared in the text with the same word just before and just after. To see what the phase is, we can use the `common_contexts` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text6.common_contexts([\"unladen\", \"african\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that both \"an unladen swallow\" and \"an african swallow\" appear in the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text6.concordance(\"unladen\")\n",
    "print()\n",
    "text6.concordance(\"african\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dispersion plot\n",
    "\n",
    "`text4` is the Inaugural Address Corpus which includes inaugural addresses going back to 1789. \n",
    "We can use a dispersion plot to see where in a text certain words appear, and hence how the language of the address has changed over time. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text4.dispersion_plot([\"citizens\", \"democracy\", \"freedom\", \"duty\", \"America\", \"nation\", \"God\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representing  language using statistics\n",
    "\n",
    "We'll represent a text by counting the frequency of different words.\n",
    "\n",
    "The total number of words (\"outcomes\") in Moby Dick is 260,819 and the number of different words is 19,317. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fdist1 = FreqDist(text1)\n",
    "print(fdist1)\n",
    "\n",
    "# find 50 most common words\n",
    "print('\\n',fdist1.most_common(50))\n",
    "\n",
    "# not suprisingly, whale occurs quite frequently (906 times!)\n",
    "print('\\n', fdist1['whale'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tricky python\n",
    "We can find all the words in Moby Dick with more than 15 characters\n",
    "\n",
    "*Note:* its faster to sort through a set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "V = set(text1)\n",
    "long_words = [w.lower() for w in V if len(w) > 15]\n",
    "sorted(long_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "It might be useful to ignore frequently used words. These are referred to as *stopwords*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Find the most frequently used words in Moby Dick which are not stopwords and not punctuation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there a difference between the frequency in which stopwords appear in the different texts? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def content_fraction(text):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    content = [w for w in text if w.lower() not in stopwords]\n",
    "    return len(content) / len(text)\n",
    "\n",
    "for i,t in enumerate([text1,text2,text3,text4,text5,text6,text7,text8,text9]):\n",
    "    print(i+1,content_fraction(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently, \"text8: Personals Corpus\" has the most content. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collocations\n",
    "A *collocation* is a sequence of words that occur together unusually often. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text8.collocations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis for movie reviews\n",
    "We ask the simple question: Is the attitude of a movie review positive or negative? \n",
    "\n",
    "A *corpus* is a large body of linguistic data. (Plural: corpora)\n",
    "\n",
    "Our data is a corpus consisting of 2000 movie reviews together with the user's sentiment polarity (positive or negative). Our goal is to predict the sentiment polarity from just the review. \n",
    "\n",
    "Of course, this is something that we can do very easily: \n",
    "1. That movie was terrible. -> negative\n",
    "+ That movie was great! -> positive\n",
    "\n",
    "More information about this dataset is available [from this website](https://www.cs.cornell.edu/people/pabo/movie-review-data/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews as reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datset contains 1000 positive and 1000 negative movie reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_reviews = len(reviews.fileids())\n",
    "print(num_reviews)\n",
    "print(len(reviews.fileids('pos')),len(reviews.fileids('neg')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the review for the third movie. Its a negative review! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the name of the file \n",
    "fid = reviews.fileids()[2]\n",
    "print(fid)\n",
    "\n",
    "print('\\n', reviews.raw(fid))\n",
    "\n",
    "print('\\n', reviews.categories(fid) )\n",
    "\n",
    "print('\\n', reviews.words(fid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis \n",
    "Goal: build a classifier that predicts the label ['neg', 'pos'] from the review text\n",
    "\n",
    "`reviews.categories(file_id)` returns the label ['neg', 'pos'] for that movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "categories = [reviews.categories(fid) for fid in reviews.fileids()]\n",
    "my_dictionary = {'pos':1, 'neg':0}\n",
    "y = [my_dictionary[x[0]] for x in categories]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_words = [list(reviews.words(fid)) for fid in reviews.fileids()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# first 10 words of the third document\n",
    "doc_words[2][1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all of the words in the reviews and make a FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_words = nltk.FreqDist(w.lower() for w in reviews.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We  define a feature for each word, indicating whether the document contains that word. To limit the total number of features, we construct a list of the 4000 most frequently appearing words in the  corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_features = 4000\n",
    "word_features = list(all_words)[:num_features]\n",
    "print(word_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function that takes a document and returns a list of zeros and ones indicating which of the words in  `word_features` appears in that document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    features = np.zeros(num_features)\n",
    "    for i,word in enumerate(word_features):\n",
    "        features[i] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just focus on the third document. Which words from `word_features` are in this document? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words_in_doc_2 = document_features(doc_words[2])\n",
    "print(words_in_doc_2)\n",
    "\n",
    "inds = np.where(words_in_doc_2 == 1)[0]\n",
    "print('\\n', [word_features[i] for i in inds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.zeros([num_reviews,num_features])\n",
    "for ii in range(num_reviews):\n",
    "    X[ii,:] = document_features(doc_words[ii])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis = Classification \n",
    "\n",
    "Now that we have features for each document and labels, we have a classification problem! \n",
    "\n",
    "NLTK has a built-in classifier, but we'll use the scikit-learn classifiers we're already familiar with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k = 5\n",
    "model = KNeighborsClassifier(n_neighbors=k)\n",
    "scores = cross_val_score(model, X, y, cv=10)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = svm.SVC(kernel='rbf',C=30)\n",
    "scores = cross_val_score(model, X, y, cv=10)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we could now use the cross validation scores to find the optimal parameters, `k` and `C`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We could have also used the Classifier from the NLTK library\n",
    "\n",
    "Below is the sentiment analysis from [Ch. 6 of the NLTK book](http://www.nltk.org/book/ch06.html). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "documents = [(list(reviews.words(fileid)), category)\n",
    "             for category in reviews.categories() \n",
    "             for fileid in reviews.fileids(category)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the features from all of the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def document_features(document):\n",
    "    # Note: checking whether a word occurs in a set is much faster \n",
    "    # than checking whether it occurs in a list     \n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "featuresets = [(document_features(d), c) for (d,c) in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into train_set, test_set and perform classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_set, test_set = featuresets[100:], featuresets[:100]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "print(nltk.classify.accuracy(classifier, test_set))\n",
    "\n",
    "classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvements? \n",
    "\n",
    "More data, better features: n-grams, part of speech tagging, ... "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
