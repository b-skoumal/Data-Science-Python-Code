{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Science, CS 5963 / Math 3900\n",
    "## Lab 12: Clustering \n",
    "\n",
    "In this lab, we'll cluster the MNIST handwritten digits using methods from the scikit-learn library. \n",
    "\n",
    "This lab is, in part, based on [this scikit-learn demo](http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import *\n",
    "# from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dataset\n",
    "\n",
    "THE MNIST handwritten digit dataset consists of images of handwritten digits, together with labels indicating which digit is in each image. \n",
    "\n",
    "Becaue both the features and the labels are present in this dataset (and labels for large datasets are generally difficult/expensive to obtain), this dataset is frequently used as a benchmark to compare various methods. \n",
    "For example, [this webpage](http://yann.lecun.com/exdb/mnist/) describes a variety of different classification results on MNIST (Note, the tests on this website are for a larger and higher resolution dataset than we'll use.) To see a comparison of classification methods implemented in scikit-learn on the MNIST dataset, see \n",
    "[this page](http://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html).\n",
    "The MNIST dataset is also a frequently used for benchmarking clustering algorithms and because it has labels, we can evaluate the homogeneity or purity of the clusters. \n",
    "\n",
    "There are several versions of the dataset. We'll use the one that is built-in to scikit-learn, described [here](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html). \n",
    "\n",
    "* Classes: 10  \n",
    "* Samples per class: $\\approx$180\n",
    "* Samples total: 1797\n",
    "* Dimensionality: 64 (8 pixels by 8 pixels)\n",
    "* Features: integers 0-16\n",
    "\n",
    "Here are some examples of the images. Note that the digits have been size-normalized and centered in a fixed-size ($8\\times8$ pixels) image.\n",
    "\n",
    "<img src=\"http://scikit-learn.org/stable/_images/sphx_glr_plot_digits_classification_001.png\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "X = scale(digits.data)\n",
    "y = digits.target\n",
    "print(type(X))\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "n_digits = len(np.unique(digits.target))\n",
    "print(\"n_digits: %d, n_samples %d, n_features %d\" % (n_digits, n_samples, n_features))\n",
    "\n",
    "plt.figure(figsize= (10, 10))\n",
    "for ii in np.arange(25):\n",
    "    plt.subplot(5, 5, ii+1)\n",
    "    plt.imshow(np.reshape(X[ii,:],(8,8)), cmap='Greys',interpolation='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-means clustering\n",
    "We first use k-means method to cluster the dataset and compute the homogeneity score for the clustering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fit k-means to data\n",
    "kmeans_model = KMeans(n_clusters=n_digits, n_init=10)\n",
    "kmeans_model.fit(X)\n",
    "\n",
    "# use labels to compute homogeneity score\n",
    "metrics.homogeneity_score(labels_true=y, labels_pred=kmeans_model.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical clustering\n",
    "**Exercise:** Use a hierarchical clustering method to cluster the dataset. Again compute the homogeneity. *Hint:* Use the scikit-learn function *AgglomerativeClustering*.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SOLUTION \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other clustering methods\n",
    "Take a look at the clustering methods and options for various methods on the [scikit-learn page](http://scikit-learn.org/stable/modules/clustering.html). \n",
    "\n",
    "**Exercise:** By modifying the following code, try to find the clustering method with the largest homogeneity score for this dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(50 * '_')\n",
    "print('% 9s' % 'method' + '                   time' + '      homo')\n",
    "def compare_method(estimator, name, data):\n",
    "    t0 = time()\n",
    "    estimator.fit(data)\n",
    "    print('% 25s   %.2fs     %.3f ' % (name, (time() - t0), metrics.homogeneity_score(y, estimator.labels_)))\n",
    "\n",
    "compare_method(KMeans(init='k-means++', n_clusters=n_digits, n_init=10), name=\"k-means++\", data=X)\n",
    "compare_method(KMeans(init='random', n_clusters=n_digits, n_init=10), name=\"random\", data=X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization via PCA\n",
    "Here we'll use PCA in conjunction with clustering to visualize a clustering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_PCA = PCA(n_components=2).fit_transform(X)\n",
    "kmeans_model = KMeans(init='k-means++', n_clusters=n_digits, n_init=10)\n",
    "kmeans_model.fit(X_PCA)\n",
    "\n",
    "# Plot the decision boundaries. For that, we will assign a color to each point in a mesh\n",
    "x_min, x_max = X_PCA[:, 0].min() - 1, X_PCA[:, 0].max() + 1\n",
    "y_min, y_max = X_PCA[:, 1].min() - 1, X_PCA[:, 1].max() + 1\n",
    "h = .02     # step size of the mesh\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "# Obtain labels for each point in mesh. Use last trained model.\n",
    "Z = kmeans_model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.imshow(Z, interpolation='nearest', \n",
    "           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "           cmap=plt.cm.Paired,\n",
    "           aspect='auto', origin='lower')\n",
    "\n",
    "plt.plot(X_PCA[:, 0], X_PCA[:, 1], 'k.', markersize=2)\n",
    "\n",
    "# Plot the centroids as a white X\n",
    "centroids = kmeans_model.cluster_centers_\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], marker='x', s=169, linewidths=3, color='w', zorder=10)\n",
    "plt.title('K-means clustering on the digits dataset (PCA-reduced data)\\n Centroids are marked with white cross')\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()\n",
    "\n",
    "print(metrics.homogeneity_score(labels_true=y, labels_pred=kmeans_model.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Plotting cluster centers and cluster representatives\n",
    "In this case, the features are just the pixels, so we can plot the cluster centers. \n",
    "\n",
    "A related idea is the notion of a cluster representative, which is not available via the k-means clustering method. A *cluster representative* or *cluster exemplar*  is an element from the cluster which \"represents\" that cluster. Representatives are a powerful idea for data summary. The affinity propagation method for clustering provides cluster representatives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fit k-means to data\n",
    "kmeans_model = KMeans(n_clusters=n_digits, n_init=10)\n",
    "kmeans_model.fit(X)\n",
    "\n",
    "# use labels to compute homogenity score\n",
    "metrics.homogeneity_score(labels_true=y, labels_pred=kmeans_model.labels_)\n",
    "\n",
    "plt.figure(figsize= (15,8))\n",
    "for ii in np.arange(10):\n",
    "    plt.subplot(2, 5, ii+1)\n",
    "    plt.imshow(np.reshape(kmeans_model.cluster_centers_[ii,:],(8,8)), cmap='Greys',interpolation='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fit affinity propogation clustering to data\n",
    "aff_prop_model = AffinityPropagation()\n",
    "aff_prop_model.fit(X)\n",
    "\n",
    "# use labels to compute homogenity score\n",
    "metrics.homogeneity_score(labels_true=y, labels_pred=aff_prop_model.labels_)\n",
    "\n",
    "print(aff_prop_model)\n",
    "\n",
    "cluster_centers_indices = aff_prop_model.cluster_centers_indices_\n",
    "print(len(cluster_centers_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot the first 20 cluster representatives\n",
    "plt.figure(figsize= (15,18))\n",
    "for ii in np.arange(20):\n",
    "    cluster_center = X[cluster_centers_indices[ii]]\n",
    "    plt.subplot(4, 5, ii+1)\n",
    "    plt.imshow(np.reshape(cluster_center,(8,8)), cmap='Greys',interpolation='none')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
